{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xtnj2UwSJ2gj7XHcYCJiLN8cjORrfO67","timestamp":1669086296619}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTMj8GSCK3h_","executionInfo":{"status":"ok","timestamp":1670119317175,"user_tz":300,"elapsed":60148,"user":{"displayName":"Kristina Yuchen Tian","userId":"02214278154750228678"}},"outputId":"7f5f99db-1fd9-45da-ef02-67de3dd75388"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (50000, 32, 32, 3)\n","50000 train samples\n","10000 test samples\n","y_train shape: (50000, 1)\n","Learning rate:  0.001\n","Model: \"model_12\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_13 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," prune_low_magnitude_conv2d_324  (None, 32, 32, 16)  882         ['input_13[0][0]']               \n","  (PruneLowMagnitude)                                                                             \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 32, 32, 16)  65          ['prune_low_magnitude_conv2d_324[\n"," alization_300 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 32, 32, 16)  1           ['prune_low_magnitude_batch_norma\n"," _300 (PruneLowMagnitude)                                        lization_300[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_325  (None, 32, 32, 16)  4626        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            300[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 32, 32, 16)  65          ['prune_low_magnitude_conv2d_325[\n"," alization_301 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 32, 32, 16)  1           ['prune_low_magnitude_batch_norma\n"," _301 (PruneLowMagnitude)                                        lization_301[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_326  (None, 32, 32, 16)  4626        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            301[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 32, 32, 16)  65          ['prune_low_magnitude_conv2d_326[\n"," alization_302 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_144 (P  (None, 32, 32, 16)  1           ['prune_low_magnitude_activation_\n"," runeLowMagnitude)                                               300[0][0]',                      \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_302[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 32, 32, 16)  1           ['prune_low_magnitude_add_144[0][\n"," _302 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_327  (None, 32, 32, 16)  4626        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            302[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 32, 32, 16)  65          ['prune_low_magnitude_conv2d_327[\n"," alization_303 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 32, 32, 16)  1           ['prune_low_magnitude_batch_norma\n"," _303 (PruneLowMagnitude)                                        lization_303[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_328  (None, 32, 32, 16)  4626        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            303[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 32, 32, 16)  65          ['prune_low_magnitude_conv2d_328[\n"," alization_304 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_145 (P  (None, 32, 32, 16)  1           ['prune_low_magnitude_activation_\n"," runeLowMagnitude)                                               302[0][0]',                      \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_304[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 32, 32, 16)  1           ['prune_low_magnitude_add_145[0][\n"," _304 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_329  (None, 32, 32, 16)  4626        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            304[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 32, 32, 16)  65          ['prune_low_magnitude_conv2d_329[\n"," alization_305 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 32, 32, 16)  1           ['prune_low_magnitude_batch_norma\n"," _305 (PruneLowMagnitude)                                        lization_305[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_330  (None, 32, 32, 16)  4626        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            305[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 32, 32, 16)  65          ['prune_low_magnitude_conv2d_330[\n"," alization_306 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_146 (P  (None, 32, 32, 16)  1           ['prune_low_magnitude_activation_\n"," runeLowMagnitude)                                               304[0][0]',                      \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_306[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 32, 32, 16)  1           ['prune_low_magnitude_add_146[0][\n"," _306 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_331  (None, 16, 16, 32)  9250        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            306[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 16, 16, 32)  129         ['prune_low_magnitude_conv2d_331[\n"," alization_307 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 16, 16, 32)  1           ['prune_low_magnitude_batch_norma\n"," _307 (PruneLowMagnitude)                                        lization_307[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_332  (None, 16, 16, 32)  18466       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            307[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_conv2d_333  (None, 16, 16, 32)  1058        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            306[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 16, 16, 32)  129         ['prune_low_magnitude_conv2d_332[\n"," alization_308 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_147 (P  (None, 16, 16, 32)  1           ['prune_low_magnitude_conv2d_333[\n"," runeLowMagnitude)                                               0][0]',                          \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_308[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 16, 16, 32)  1           ['prune_low_magnitude_add_147[0][\n"," _308 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_334  (None, 16, 16, 32)  18466       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            308[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 16, 16, 32)  129         ['prune_low_magnitude_conv2d_334[\n"," alization_309 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 16, 16, 32)  1           ['prune_low_magnitude_batch_norma\n"," _309 (PruneLowMagnitude)                                        lization_309[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_335  (None, 16, 16, 32)  18466       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            309[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 16, 16, 32)  129         ['prune_low_magnitude_conv2d_335[\n"," alization_310 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_148 (P  (None, 16, 16, 32)  1           ['prune_low_magnitude_activation_\n"," runeLowMagnitude)                                               308[0][0]',                      \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_310[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 16, 16, 32)  1           ['prune_low_magnitude_add_148[0][\n"," _310 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_336  (None, 16, 16, 32)  18466       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            310[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 16, 16, 32)  129         ['prune_low_magnitude_conv2d_336[\n"," alization_311 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 16, 16, 32)  1           ['prune_low_magnitude_batch_norma\n"," _311 (PruneLowMagnitude)                                        lization_311[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_337  (None, 16, 16, 32)  18466       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            311[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 16, 16, 32)  129         ['prune_low_magnitude_conv2d_337[\n"," alization_312 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_149 (P  (None, 16, 16, 32)  1           ['prune_low_magnitude_activation_\n"," runeLowMagnitude)                                               310[0][0]',                      \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_312[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 16, 16, 32)  1           ['prune_low_magnitude_add_149[0][\n"," _312 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_338  (None, 8, 8, 64)    36930       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            312[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 8, 8, 64)    257         ['prune_low_magnitude_conv2d_338[\n"," alization_313 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 8, 8, 64)    1           ['prune_low_magnitude_batch_norma\n"," _313 (PruneLowMagnitude)                                        lization_313[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_339  (None, 8, 8, 64)    73794       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            313[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_conv2d_340  (None, 8, 8, 64)    4162        ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            312[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 8, 8, 64)    257         ['prune_low_magnitude_conv2d_339[\n"," alization_314 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_150 (P  (None, 8, 8, 64)    1           ['prune_low_magnitude_conv2d_340[\n"," runeLowMagnitude)                                               0][0]',                          \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_314[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 8, 8, 64)    1           ['prune_low_magnitude_add_150[0][\n"," _314 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_341  (None, 8, 8, 64)    73794       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            314[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 8, 8, 64)    257         ['prune_low_magnitude_conv2d_341[\n"," alization_315 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 8, 8, 64)    1           ['prune_low_magnitude_batch_norma\n"," _315 (PruneLowMagnitude)                                        lization_315[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_342  (None, 8, 8, 64)    73794       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            315[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 8, 8, 64)    257         ['prune_low_magnitude_conv2d_342[\n"," alization_316 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_151 (P  (None, 8, 8, 64)    1           ['prune_low_magnitude_activation_\n"," runeLowMagnitude)                                               314[0][0]',                      \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_316[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 8, 8, 64)    1           ['prune_low_magnitude_add_151[0][\n"," _316 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_343  (None, 8, 8, 64)    73794       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            316[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 8, 8, 64)    257         ['prune_low_magnitude_conv2d_343[\n"," alization_317 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 8, 8, 64)    1           ['prune_low_magnitude_batch_norma\n"," _317 (PruneLowMagnitude)                                        lization_317[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_conv2d_344  (None, 8, 8, 64)    73794       ['prune_low_magnitude_activation_\n","  (PruneLowMagnitude)                                            317[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_batch_norm  (None, 8, 8, 64)    257         ['prune_low_magnitude_conv2d_344[\n"," alization_318 (PruneLowMagnitu                                  0][0]']                          \n"," de)                                                                                              \n","                                                                                                  \n"," prune_low_magnitude_add_152 (P  (None, 8, 8, 64)    1           ['prune_low_magnitude_activation_\n"," runeLowMagnitude)                                               316[0][0]',                      \n","                                                                  'prune_low_magnitude_batch_norma\n","                                                                 lization_318[0][0]']             \n","                                                                                                  \n"," prune_low_magnitude_activation  (None, 8, 8, 64)    1           ['prune_low_magnitude_add_152[0][\n"," _318 (PruneLowMagnitude)                                        0]']                             \n","                                                                                                  \n"," prune_low_magnitude_average_po  (None, 1, 1, 64)    1           ['prune_low_magnitude_activation_\n"," oling2d_12 (PruneLowMagnitude)                                  318[0][0]']                      \n","                                                                                                  \n"," prune_low_magnitude_flatten_12  (None, 64)          1           ['prune_low_magnitude_average_poo\n","  (PruneLowMagnitude)                                            ling2d_12[0][0]']                \n","                                                                                                  \n"," prune_low_magnitude_dense_12 (  (None, 10)          1292        ['prune_low_magnitude_flatten_12[\n"," PruneLowMagnitude)                                              0][0]']                          \n","                                                                                                  \n","==================================================================================================\n","Total params: 545,431\n","Trainable params: 273,066\n","Non-trainable params: 272,365\n","__________________________________________________________________________________________________\n","ResNet20v1\n","Not using data augmentation.\n","Learning rate:  0.001\n","Epoch 1/2\n","  6/391 [..............................] - ETA: 23s - loss: 2.9404 - accuracy: 0.1367"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0347s vs `on_train_batch_end` time: 0.0890s). Check your callbacks.\n"]},{"output_type":"stream","name":"stdout","text":["390/391 [============================>.] - ETA: 0s - loss: 1.6693 - accuracy: 0.4408"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r391/391 [==============================] - 36s 50ms/step - loss: 1.6687 - accuracy: 0.4409 - val_loss: 1.5600 - val_accuracy: 0.4746 - lr: 0.0010\n","Learning rate:  0.001\n","Epoch 2/2\n","391/391 [==============================] - ETA: 0s - loss: 1.2977 - accuracy: 0.5695"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r391/391 [==============================] - 18s 47ms/step - loss: 1.2977 - accuracy: 0.5695 - val_loss: 1.2251 - val_accuracy: 0.5865 - lr: 0.0010\n","overall training time is 57.407480001449585\n","each epoch training time is [35.56071901321411, 18.509753704071045]\n"]}],"source":["Oh and clickfrom __future__ import print_function\n","import keras\n","import tempfile\n","from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import CSVLogger  #, UpdatePruningStep\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.models import Model\n","from keras.datasets import cifar10\n","from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n","import tensorflow_model_optimization as tfmot\n","import numpy as np\n","import time\n","import os\n","\n","\n","class TimeHistory(keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.times = []\n","\n","    def on_epoch_begin(self, batch, logs={}):\n","        self.epoch_time_start = time.time()\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        self.times.append(time.time() - self.epoch_time_start)\n","\n","time_callback = TimeHistory()\n","\n","\n","\n","#logger \n","csv_logger = CSVLogger('log_20layers_V100.csv', append=True, separator=';')\n","\n","# Training parameters\n","batch_size = 128  # orig paper trained all networks with batch_size=128\n","epochs = 2\n","data_augmentation = False #True\n","num_classes = 10\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Model parameter\n","# ----------------------------------------------------------------------------\n","#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n","# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n","#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n","# ----------------------------------------------------------------------------\n","# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n","# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n","# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n","# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n","# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n","# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n","# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n","# ---------------------------------------------------------------------------\n","n = 3\n","\n","# Model version\n","# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n","version = 1\n","\n","# Computed depth from supplied model parameter n\n","if version == 1:\n","    depth = n * 6 + 2\n","elif version == 2:\n","    depth = n * 9 + 2\n","\n","# Model name, depth and version\n","model_type = 'ResNet%dv%d' % (depth, version)\n","\n","# Load the CIFAR10 data.\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","# Input image dimensions.\n","input_shape = x_train.shape[1:]\n","\n","# Normalize data.\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","if subtract_pixel_mean:\n","    x_train_mean = np.mean(x_train, axis=0)\n","    x_train -= x_train_mean\n","    x_test -= x_train_mean\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","print('y_train shape:', y_train.shape)\n","\n","# Convert class vectors to binary class matrices.\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","\n","def resnet_layer(inputs,\n","                 num_filters=16,\n","                 kernel_size=3,\n","                 strides=1,\n","                 activation='relu',\n","                 batch_normalization=True,\n","                 conv_first=True):\n","    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n","\n","    # Arguments\n","        inputs (tensor): input tensor from input image or previous layer\n","        num_filters (int): Conv2D number of filters\n","        kernel_size (int): Conv2D square kernel dimensions\n","        strides (int): Conv2D square stride dimensions\n","        activation (string): activation name\n","        batch_normalization (bool): whether to include batch normalization\n","        conv_first (bool): conv-bn-activation (True) or\n","            bn-activation-conv (False)\n","\n","    # Returns\n","        x (tensor): tensor as input to the next layer\n","    \"\"\"\n","    conv = Conv2D(num_filters,\n","                  kernel_size=kernel_size,\n","                  strides=strides,\n","                  padding='same',\n","                  kernel_initializer='he_normal',\n","                  kernel_regularizer=l2(1e-4))\n","\n","    x = inputs\n","    if conv_first:\n","        x = conv(x)\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","    else:\n","        if batch_normalization:\n","            x = BatchNormalization()(x)\n","        if activation is not None:\n","            x = Activation(activation)(x)\n","        x = conv(x)\n","    return x\n","\n","\n","def resnet_v1(input_shape, depth, num_classes=10):\n","    \"\"\"ResNet Version 1 Model builder [a]\n","\n","    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n","    Last ReLU is after the shortcut connection.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filters is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same number of filters.\n","    Features maps sizes:\n","    stage 0: 32x32, 16\n","    stage 1: 16x16, 32\n","    stage 2:  8x8,  64\n","    The Number of parameters is approx the same as Table 6 of [a]:\n","    ResNet20 0.27M\n","    ResNet32 0.46M\n","    ResNet44 0.66M\n","    ResNet56 0.85M\n","    ResNet110 1.7M\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 6 != 0:\n","        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n","    # Start model definition.\n","    num_filters = 16\n","    num_res_blocks = int((depth - 2) / 6)\n","\n","    inputs = Input(shape=input_shape)\n","    x = resnet_layer(inputs=inputs)\n","    # Instantiate the stack of residual units\n","    for stack in range(3):\n","        for res_block in range(num_res_blocks):\n","            strides = 1\n","            if stack > 0 and res_block == 0:  # first layer but not first stack\n","                strides = 2  # downsample\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters,\n","                             strides=strides)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters,\n","                             activation=None)\n","            if stack > 0 and res_block == 0:  # first layer but not first stack\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","            x = Activation('relu')(x)\n","        num_filters *= 2\n","\n","    # Add classifier on top.\n","    # v1 does not use BN after last shortcut connection-ReLU\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","\n","def resnet_v2(input_shape, depth, num_classes=10):\n","    \"\"\"ResNet Version 2 Model builder [b]\n","\n","    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n","    bottleneck layer\n","    First shortcut connection per layer is 1 x 1 Conv2D.\n","    Second and onwards shortcut connection is identity.\n","    At the beginning of each stage, the feature map size is halved (downsampled)\n","    by a convolutional layer with strides=2, while the number of filter maps is\n","    doubled. Within each stage, the layers have the same number filters and the\n","    same filter map sizes.\n","    Features maps sizes:\n","    conv1  : 32x32,  16\n","    stage 0: 32x32,  64\n","    stage 1: 16x16, 128\n","    stage 2:  8x8,  256\n","\n","    # Arguments\n","        input_shape (tensor): shape of input image tensor\n","        depth (int): number of core convolutional layers\n","        num_classes (int): number of classes (CIFAR10 has 10)\n","\n","    # Returns\n","        model (Model): Keras model instance\n","    \"\"\"\n","    if (depth - 2) % 9 != 0:\n","        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n","    # Start model definition.\n","    num_filters_in = 16\n","    num_res_blocks = int((depth - 2) / 9)\n","\n","    inputs = Input(shape=input_shape)\n","    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n","    x = resnet_layer(inputs=inputs,\n","                     num_filters=num_filters_in,\n","                     conv_first=True)\n","\n","    # Instantiate the stack of residual units\n","    for stage in range(3):\n","        for res_block in range(num_res_blocks):\n","            activation = 'relu'\n","            batch_normalization = True\n","            strides = 1\n","            if stage == 0:\n","                num_filters_out = num_filters_in * 4\n","                if res_block == 0:  # first layer and first stage\n","                    activation = None\n","                    batch_normalization = False\n","            else:\n","                num_filters_out = num_filters_in * 2\n","                if res_block == 0:  # first layer but not first stage\n","                    strides = 2    # downsample\n","\n","            # bottleneck residual unit\n","            y = resnet_layer(inputs=x,\n","                             num_filters=num_filters_in,\n","                             kernel_size=1,\n","                             strides=strides,\n","                             activation=activation,\n","                             batch_normalization=batch_normalization,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_in,\n","                             conv_first=False)\n","            y = resnet_layer(inputs=y,\n","                             num_filters=num_filters_out,\n","                             kernel_size=1,\n","                             conv_first=False)\n","            if res_block == 0:\n","                # linear projection residual shortcut connection to match\n","                # changed dims\n","                x = resnet_layer(inputs=x,\n","                                 num_filters=num_filters_out,\n","                                 kernel_size=1,\n","                                 strides=strides,\n","                                 activation=None,\n","                                 batch_normalization=False)\n","            x = keras.layers.add([x, y])\n","\n","        num_filters_in = num_filters_out\n","\n","    # Add classifier on top.\n","    # v2 has BN-ReLU before Pooling\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = AveragePooling2D(pool_size=8)(x)\n","    y = Flatten()(x)\n","    outputs = Dense(num_classes,\n","                    activation='softmax',\n","                    kernel_initializer='he_normal')(y)\n","\n","    # Instantiate model.\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","\n","if version == 2:\n","    model = resnet_v2(input_shape=input_shape, depth=depth)\n","else:\n","    model = resnet_v1(input_shape=input_shape, depth=depth)\n","\n","\n","####Changes start#####\n","num_images = x_train.shape[0] #* (1 - validation_split)\n","end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n","\n","\n","\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","\n","#hyperparameters: initial_sparsity=0.50, final_sparsity=0.80\n","\n","pruning_params = {\n","      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n","                                                               final_sparsity=0.80,\n","                                                               begin_step=0,\n","                                                               end_step=end_step)\n","}\n","\n","model = prune_low_magnitude(model, **pruning_params)    #_for_pruning\n","\n","####Changes end#####\n","#''  tf.  keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","model.compile(loss= 'categorical_crossentropy',     \n","              optimizer=Adam(lr=lr_schedule(0)),\n","              metrics=['accuracy'])\n","model.summary()\n","print(model_type)\n","\n","# Prepare model model saving directory.\n","save_dir = os.path.join(os.getcwd(), 'saved_models')\n","model_name = f'cifar10_%s_model_V100.h5' % model_type\n","if not os.path.isdir(save_dir):\n","    os.makedirs(save_dir)\n","filepath = os.path.join(save_dir, model_name)\n","\n","# Prepare callbacks for model saving and for learning rate adjustment.\n","checkpoint = ModelCheckpoint(filepath=filepath,\n","                             monitor='val_acc',\n","                             verbose=1,\n","                             save_best_only=True)\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n","                               cooldown=0,\n","                               patience=5,\n","                               min_lr=0.5e-6)\n","logdir = tempfile.mkdtemp()\n","callbacks = [checkpoint, lr_reducer, lr_scheduler, csv_logger, time_callback, pruning_callbacks.UpdatePruningStep(), tfmot.sparsity.keras.PruningSummaries(log_dir=logdir)]\n","\n","# Run training, with or without data augmentation.\n","if not data_augmentation:\n","    print('Not using data augmentation.')\n","    st = time.time()\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              validation_data=(x_test, y_test),\n","              shuffle=True,\n","              callbacks=callbacks)\n","    training_time = time.time() - st\n","else:\n","    print('Using real-time data augmentation.')\n","    # This will do preprocessing and realtime data augmentation:\n","    datagen = ImageDataGenerator(\n","        # set input mean to 0 over the dataset\n","        featurewise_center=False,\n","        # set each sample mean to 0\n","        samplewise_center=False,\n","        # divide inputs by std of dataset\n","        featurewise_std_normalization=False,\n","        # divide each input by its std\n","        samplewise_std_normalization=False,\n","        # apply ZCA whitening\n","        zca_whitening=False,\n","        # epsilon for ZCA whitening\n","        zca_epsilon=1e-06,\n","        # randomly rotate images in the range (deg 0 to 180)\n","        rotation_range=0,\n","        # randomly shift images horizontally\n","        width_shift_range=0.1,\n","        # randomly shift images vertically\n","        height_shift_range=0.1,\n","        # set range for random shear\n","        shear_range=0.,\n","        # set range for random zoom\n","        zoom_range=0.,\n","        # set range for random channel shifts\n","        channel_shift_range=0.,\n","        # set mode for filling points outside the input boundaries\n","        fill_mode='nearest',\n","        # value used for fill_mode = \"constant\"\n","        cval=0.,\n","        # randomly flip images\n","        horizontal_flip=True,\n","        # randomly flip images\n","        vertical_flip=False,\n","        # set rescaling factor (applied before any other transformation)\n","        rescale=None,\n","        # set function that will be applied on each input\n","        preprocessing_function=None,\n","        # image data format, either \"channels_first\" or \"channels_last\"\n","        data_format=None,\n","        # fraction of images reserved for validation (strictly between 0 and 1)\n","        validation_split=0.0)\n","\n","    # Compute quantities required for featurewise normalization\n","    # (std, mean, and principal components if ZCA whitening is applied).\n","    datagen.fit(x_train)\n","\n","    st = time.time()\n","    # Fit the model on the batches generated by datagen.flow().\n","    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                        validation_data=(x_test, y_test),\n","                        epochs=epochs, verbose=1, workers=4,\n","                        callbacks=callbacks)\n","    training_time = time.time() - st\n","print(f\"overall training time is {training_time}\")\n","times = time_callback.times\n","print(f\"each epoch training time is {times}\")\n","\n","\n","# Score trained model.\n","#scores = model.evaluate(x_test, y_test, verbose=1)\n","#print('Test loss:', scores[0])\n","#print('Test accuracy:', scores[1])\n","model.save(save_dir+'/'+model_name)"]},{"cell_type":"code","source":["import tensorflow_model_optimization as tfmot\n","\n","prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n","\n","# Compute end step to finish pruning after 2 epochs.\n","batch_size = 128\n","epochs = 2\n","validation_split = 0.1 # 10% of training set will be used for validation set. \n","\n","num_images = train_images.shape[0] * (1 - validation_split)\n","end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n","\n","# Define model for pruning.\n","pruning_params = {\n","      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n","                                                               final_sparsity=0.80,\n","                                                               begin_step=0,\n","                                                               end_step=end_step)\n","}\n","\n","model_for_pruning = prune_low_magnitude(model, **pruning_params)\n","\n","# `prune_low_magnitude` requires a recompile.\n","model_for_pruning.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","model_for_pruning.summary()"],"metadata":{"id":"s9wvD3gsh-Nk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" !pip install -q tensorflow-model-optimization"],"metadata":{"id":"040pm_YXiTLX"},"execution_count":null,"outputs":[]}]}